pip install -e ".[torch,metrics]"
pip install "deepspeed>=0.10.0,<=0.16.9" wandb

# A5000 26GB Out of memory full sft
FORCE_TORCHRUN=1 llamafactory-cli train examples/train_full/llama3_full_sft.yaml

llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
# original script
# training_epoch 3 -> 2, learning 1.0e-4 -> 5.0e-4
# training_epoch 3 -> 2
# rank 8 -> 4

## above success
FORCE_TORCHRUN=1 llamafactory-cli train examples/train_full/llama3_full_sft.yaml
# evaluate
llamafactory-cli train examples/extras/nlg_eval/llama3_lora_predict.yaml


# Using `or_mask_function` or `and_mask_function` arguments require torch>=2.6
# Train google/gemma-3-4b-it with LoRA method.
train examples/train_lora/llama3_lora_sft.yaml model_name_or_path=google/gemma-3-4b-it dataset=train_gemma3 max_samples=32768 cutoff_len=32768 output_dir=saves/gemma-3-4b/lora/sft num_train_epochs=1.0 template=gemma3

# Train google/gemma-3-270m-it with Full-SFT methods.
FORCE_TORCHRUN=1 llamafactory-cli train examples/train_full/llama3_full_sft.yaml model_name_or_path=google/gemma-3-270m-it dataset=train_gemma3 max_samples=32768 cutoff_len=32768 output_dir=saves/gemma-3-270m/full/sft num_train_epochs=1.0 deepspeed=examples/deepspeed/ds_z0_config.json





